{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "\n",
    "This project illustrates various stages of data wrangling and done as a part of the coursework for data analyst nano degree from Udacity.\n",
    "\n",
    "## Description of Data\n",
    "\n",
    "The data chosen for this project is downloaded as a OSM XML zip file (19Mb - Unzipped: 295Mb) from https://mapzen.com/data/metro-extracts/metro/nashville_tennessee/. Nashville area in Tennessee is chosen for the limited computing power of the  machine. Before inserting the cleaned bulk data into database to perform queries, an initial screening is done follwed by auditing specific fields and cleaning the data.\n",
    "\n",
    "## Inital Screening\n",
    "The initial screening includes programtically screening data to understand the frequency of different tags and tag-attributes in `XML` file. Iterative parsing of `XML` is performed using the package `xml.etree.cElementTree` by looping through each tags. The function `count_tags` prints the frequency of the tags and `tag_attrib` prints the various attributes in each tag as a dictionary of sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bounds': 1,\n",
       " 'member': 16816,\n",
       " 'nd': 1499455,\n",
       " 'node': 1317086,\n",
       " 'osm': 1,\n",
       " 'relation': 1897,\n",
       " 'tag': 919289,\n",
       " 'way': 135617}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Counting the number of various tags in the file.\n",
    "\"\"\"\n",
    "# filename=open(\"test.xml\")\n",
    "filename=open(\"nashville_tennessee.osm\",\"r\")\n",
    "\n",
    "def count_tags(filename):\n",
    "    tags = {}\n",
    "    # iterative parsing of tags\n",
    "    for event, elem in ET.iterparse(filename, events=(\"start\",)):\n",
    "        #increment for tags\n",
    "        if elem.tag not in tags:\n",
    "            tags[elem.tag] = 1\n",
    "        else:\n",
    "            tags[elem.tag] += 1\n",
    "    return tags\n",
    "\n",
    "count_tags(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'set'>,\n",
      "            {'bounds': {'minlat', 'maxlon', 'minlon', 'maxlat'},\n",
      "             'member': {'type', 'role', 'ref'},\n",
      "             'nd': {'ref'},\n",
      "             'node': {'changeset',\n",
      "                      'id',\n",
      "                      'lat',\n",
      "                      'lon',\n",
      "                      'timestamp',\n",
      "                      'uid',\n",
      "                      'user',\n",
      "                      'version'},\n",
      "             'osm': {'version', 'timestamp', 'generator'},\n",
      "             'relation': {'changeset',\n",
      "                          'id',\n",
      "                          'timestamp',\n",
      "                          'uid',\n",
      "                          'user',\n",
      "                          'version'},\n",
      "             'tag': {'v', 'k'},\n",
      "             'way': {'changeset', 'version', 'uid', 'id', 'user', 'timestamp'}})\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The various attributes for each tag\n",
    "\"\"\"\n",
    "filename=open(\"nashville_tennessee.osm\",\"r\")\n",
    "\n",
    "def tag_attribs(filename):\n",
    "    tag_attrib = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(filename, events=(\"start\",)):\n",
    "        for e in elem.attrib:\n",
    "            tag_attrib[elem.tag].add(e)\n",
    "    return tag_attrib\n",
    "audit_tag=tag_attribs(filename)\n",
    "pprint.pprint(audit_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing Data\n",
    "The various tags and attributes are inspected further to find that most of the information is stored in the tag \"tag\" as `keys[\"k\"]` and `values[\"v\"]`. Function `unique_keys` is defined to aggregate various key values to decide which data has to be cleaned to make a data structure to be passed to database. 3 other major types of tags are found to be `node, way and relation` with each having attributes related to its creation. Since printing the lengthy outputs are impractial, `write_dict` function is defined to print the output to a `.txt` file in the current directory.\n",
    "\n",
    "Auditing of data reveals that\n",
    "\n",
    "1. Some `Postcodes` are prefixed with `\"TN\"` and has to be cleaened to a neat 5-digit values\n",
    "2. Phone numbers are in various formats with some including extensions and some lacking area code. In the absense of area code, the phone number has to be rejected. Those with extension numbers has to be stripped from the end.\n",
    "3. `State` for nashville has to be cleaned to `\"TN\"` for uiformity\n",
    "4. `Amenities` can be passed on to database with out cleaning\n",
    "5. `City` names have to be cleaned for uniformity, some city names with lower caps in the begining of the sentence has to be changed to uppercase.\n",
    "6. `Country`  has to be uniformly cleaned to `\"USA\"`\n",
    "7. `Neighbourhood` fields doesn't have to be changed\n",
    "8. `Street` names has to be fixed by changing the abbreviated version to the longform, eg: `\"Ave.\"` to `\"Avenue\"`, `\"Dr.\"` to `\"Drive\"`\n",
    "9. There is not enough validated daa to fix `housenumbers` and `housenames`, hence not fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Auditing various fields in the data set to find inconsistencies\n",
    "\"\"\"\n",
    "\n",
    "filename=open(\"nashville_tennessee.osm\",\"r\")\n",
    "def audit_tags(filename):\n",
    "    #Regular Expressions\n",
    "    pincode=re.compile(r'^([0-9]{5})(?:-[0-9]{4})?$')\n",
    "    phone=re.compile(r\"^\\(?([0-9]{3})\\)?[-. ]?([0-9]{3})[-. ]?([0-9]{4})$\")\n",
    "    street_name_re=re.compile(r\"\\b\\S+\\.?$\", re.IGNORECASE)\n",
    "    \n",
    "    # fields in tags to be audited\n",
    "    POST_FIELDS=[\"addr:postcode\", \"postal_code\"]\n",
    "    CITIES_FIELDS = [\"addr:city\", \"is_in:city\"]\n",
    "    PHONE_FIELDS=[\"Phone\",\"phone\"]#,'contact:phone'],\n",
    "                  #'telephone','communication:mobile_phone',\n",
    "                  #'phone:local', 'disused:phone']\n",
    "    STREET_FIELDS=['addr:street','destination:street']\n",
    "    EXPECTED_STREET_NAMES = [\"Avenue\", \"Court\", \"Lane\", \"Boulevard\", \"Drive\",\n",
    "                             \"Court\", \"Place\",\"Road\",\"Parkway\",\"Circle\",\n",
    "                             \"South\",\"North\",\"Highway\",\"Trail\",\"Terrace\",\n",
    "                             \"Square\", \"Pike\",\"Alley\",\"Street\",\"Trace\",\n",
    "                             \"Bypass\",\"Way\",\"Fork\", \"Plaza\",\"Broadway\", \n",
    "                             \"Loop\", \"Cove\",\"Flagpole\", \"Foxborough\", \n",
    "                             \"Foxland\", \"Center\", \"Hollow\",\"East\",\"Heights\",\n",
    "                             \"Landing\", \"Springs\",\"Hills\", \"Mission\"]\n",
    "    bad_postcodes=set()\n",
    "    bad_states=set()\n",
    "    cities=set()\n",
    "    countries=set()\n",
    "    bad_phones=set()\n",
    "    good_phones=set()\n",
    "    bad_street_names=defaultdict(set)\n",
    "    amenities=set()\n",
    "    names=set()\n",
    "    \n",
    "    for event, elem in ET.iterparse(filename, events=(\"start\",)):\n",
    "        if elem.tag == \"tag\": \n",
    "            if elem.attrib[\"k\"] in POST_FIELDS:\n",
    "                code=elem.attrib[\"v\"]\n",
    "                m=pincode.match(code)\n",
    "                if not m:\n",
    "                    bad_postcodes.add(code)\n",
    "            if elem.attrib[\"k\"]==\"addr:state\" and elem.attrib[\"v\"]!=\"TN\":\n",
    "                bad_states.add(elem.attrib[\"v\"])\n",
    "            if elem.attrib[\"k\"]==\"amenity\":\n",
    "                amenities.add(elem.attrib[\"v\"])\n",
    "            if elem.attrib[\"k\"] in CITIES_FIELDS:\n",
    "                cities.add(elem.attrib[\"v\"])\n",
    "            if elem.attrib[\"k\"]==\"addr:country\":\n",
    "                countries.add(elem.attrib[\"v\"])\n",
    "            if elem.attrib[\"k\"]==\"name\":\n",
    "                names.add(elem.attrib[\"v\"])\n",
    "            if elem.attrib[\"k\"] in PHONE_FIELDS:\n",
    "                phone_num=elem.attrib[\"v\"].lstrip(\"+1- \")\n",
    "                m=re.match(phone,phone_num)\n",
    "                if m:\n",
    "                    good_phones.add(m.groups())\n",
    "                if not m:\n",
    "                    bad_phones.add(phone_num)\n",
    "            if elem.attrib[\"k\"] in STREET_FIELDS:\n",
    "                m=street_name_re.search(elem.attrib[\"v\"])\n",
    "                if m:\n",
    "                    street_type=m.group()\n",
    "                    if street_type not in EXPECTED_STREET_NAMES:\n",
    "                        bad_street_names[street_type].add(elem.attrib[\"v\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Inserting to Database\n",
    "\n",
    "Various functions are defined to clean the fields identified in data through auditing and the following functions are performed. An instance of `mongodb` has to be running before executing the code below.\n",
    "\n",
    "   1. `Phone` number is extracted as a 3-tuple and phone numbers with missing data is discarded.\n",
    "   2. Zip codes are cleaned to a 5-digit sequence ensure data integrity.\n",
    "   3. Street names are cleaned if not in the expected names, to use the longform street name. A dictionary is used for mapping the problematic fields to required clean fields.\n",
    "   4. `Country` and state fields are changed if address field exists.\n",
    "   5. `City` names and street names are cleaned based on a mapping dictionary.\n",
    "   6. `References` to other tags are added as a list\n",
    "   7. `address`, `created` and `ref` fields are added only if exists.\n",
    "\n",
    "\n",
    "Cleaned data is made into a nested data structure with fields grouped as address, created etc. The structure is defined in function `shape_data`. Data is then passed to a JSON file using `data_to_json`. Finally a database `openstreetmap` is fetched in the `test` and `.json` is inserted as a collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions used to clean various fields. \n",
    "Take in data and returns cleaned values.\n",
    "\"\"\"\n",
    "\n",
    "def clean_phone(number):\n",
    "    # returns phone numbers after cleaning as 3-tuple: (xxx,xxx,xxxx)\n",
    "    # discarded numbers are printed\n",
    "    try:\n",
    "        phone_re=re.compile(r\"^\\(?([0-9]{3})\\)?[-. ]?([0-9]{3})[-. ]?([0-9]{4})$\")\n",
    "        m=phone_re.match(number.lstrip(\"+1- \").strip())\n",
    "        phone=\"1 ({0})-{1}-{2}\".format(m.groups(0)[0],m.groups(0)[1], m.groups(0)[2])\n",
    "        return phone\n",
    "    except AttributeError:\n",
    "        print(\"From clean_phone, Bad number\\t:\",number) # Uncomment to print bad phone\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cleaning data to pass to database\n",
    "=================================\n",
    "A general structure of data:\n",
    "   {\n",
    "    'address': {'neighbourhood': ...\n",
    "                'country': 'USA', \n",
    "                'state': 'TN', \n",
    "                'street': 'Old Rocky Fork'},\n",
    "     'created': {'changeset': '4470981',\n",
    "                 'timestamp': '2010-04-19T18:58:26Z',\n",
    "                 'uid': '270262',\n",
    "                 'user': 'Ab Ye',\n",
    "                 'version': '1'},    \n",
    "     'e_id': '702177640',\n",
    "     'name': 'Nolensville Ball Park',\n",
    "     'pos': [35.9544391, -86.6665184],\n",
    "     'type': 'node'\n",
    "     'phone' : [\"XXX\",\"XXX\",\"XXXX\"]\n",
    "     'amenity': \"\"\n",
    "     'ref':[345534,534534,534534] \n",
    "    } \n",
    "     # references are added from  <\"nd\"> for <\"way\"> && <\"member\"> for <\"relation\">\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def shape_data(element):\n",
    "    if element.tag in [\"relation\", \"way\", \"node\"]:\n",
    "        data={}\n",
    "        data[\"type\"]=element.tag\n",
    "        data[\"e_id\"]=element.attrib[\"id\"]\n",
    "        \n",
    "        created={}\n",
    "        for field in [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]:\n",
    "            created[field]=element.attrib[field]\n",
    "        if created!={}:\n",
    "            data[\"created\"]=created\n",
    "        \n",
    "        if element.tag==\"node\": # add position as list : pos=[lat,lon]\n",
    "            pos=[0,0]\n",
    "            pos[0]=float(element.attrib[\"lat\"])\n",
    "            pos[1]=float(element.attrib[\"lon\"])\n",
    "            data[\"pos\"]=pos\n",
    "        \n",
    "        address={} # initializing address field\n",
    "        for elem in element.iter(\"tag\"):\n",
    "            if elem.attrib[\"k\"]==\"addr:postcode\": #add cleaned postcode\n",
    "                postcode=clean_pincode(elem.attrib[\"v\"])\n",
    "                if postcode:\n",
    "                    address[\"postcode\"]=postcode\n",
    "            if elem.attrib[\"k\"]==\"addr:street\": # add street after mapping\n",
    "                if elem.attrib[\"v\"]:\n",
    "                    address[\"street\"]=clean_streetnames(elem.attrib[\"v\"])\n",
    "            if elem.attrib[\"k\"]==\"addr:city\": # add city after mapping\n",
    "                if elem.attrib[\"v\"]:\n",
    "                    address[\"city\"]=clean_city(elem.attrib[\"v\"])\n",
    "            if elem.attrib[\"k\"] in [\"Phone\",\"phone\"]:# adding phone number\n",
    "                phone = clean_phone(elem.attrib[\"v\"])\n",
    "                if phone:\n",
    "                    data[\"phone\"]=phone\n",
    "        \n",
    "        if address != {}: # Append address if not empty\n",
    "            address[\"state\"]=\"TN\" # Adding state field\n",
    "            address[\"country\"]=\"USA\" #Adding country field\n",
    "            data[\"address\"]=address\n",
    "        \n",
    "        ref=[]\n",
    "        for member in element.iter(\"member\"):\n",
    "            ref.append(member.attrib[\"ref\"])\n",
    "        if ref!=[] and element.tag==\"relation\":\n",
    "            data[\"ref\"]=ref\n",
    "        \n",
    "        for nd in element.iter(\"nd\"):\n",
    "            ref.append(nd.attrib[\"ref\"])\n",
    "        if ref!=[] and element.tag==\"way\":\n",
    "            data[\"ref\"]=ref \n",
    "    \n",
    "        return data\n",
    "\n",
    "def get_db(db_name): \n",
    "    \"\"\" initiating client and returns db from the Case Study scripts\"\"\"\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def test():\n",
    "    file_in=open(\"nashville_tennessee.osm\",\"r\")\n",
    "    file_out=\"cleaned_data_{0}.json\".format(file_in.name[:10])\n",
    "\n",
    "#     Generating JSON data from osm file\n",
    "    cleaned_data=data_to_json(file_in, file_out)\n",
    "    db=get_db(\"openstreetmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Queries\n",
    "\n",
    "After inserting data into the database, queries are performed to answer various. `get_db` is defined to access the data basto perform needed queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview\n",
    ">**File Sizes**\n",
    "\n",
    "  >* nashville_tennessee.osm...........294.8Mb\n",
    "  >* cleaned_data\\_nashville\\_.json.....312.4Mb\n",
    "\n",
    "This section contains basic statistics about the dataset and the MongoDB queries used to gather them. The required queries to meet the specifiation of project is followed by additional suggestions to improve the dataset and additional queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries in the collection Nashville\t: 1454600\n",
      "Total number of nodes in Nashville\t: 1317086\n",
      "Total number of ways in Nashville\t: 135617\n",
      "Total number of unique users\t\t: 1025\n",
      "Top contributing user\t\t:woodpeck_fixbot, (no. of entries:278223)\n",
      "Number of contributing users appearing only once\t: 187\n"
     ]
    }
   ],
   "source": [
    "# total number of entries in the nashville collection\n",
    "total_queries=db.nashville.find().count()\n",
    "print(\"Total number of entries in the collection Nashville\\t:\", total_queries)\n",
    "# total number of nodes in the nashville collection\n",
    "nodes=db.nashville.find({\"type\":\"node\"}).count()\n",
    "print(\"Total number of nodes in Nashville\\t:\", nodes)\n",
    "# total number of nodes in the nashville collection\n",
    "way=db.nashville.find({\"type\":\"way\"}).count()\n",
    "print(\"Total number of ways in Nashville\\t:\", way)\n",
    "# number of unique users in the nashville collection\n",
    "un=len(db.nashville.distinct(\"created.user\"))\n",
    "print(\"Total number of unique users\\t\\t:\", un)\n",
    "# Top contributing user\n",
    "pipeline1 = [\n",
    "        {\"$group\":{\"_id\":\"$created.user\",\n",
    "                    \"count\":{\"$sum\":1}}},\n",
    "        {\"$sort\" :{\"count\":-1}},\n",
    "        {\"$limit\":1}\n",
    "        ]\n",
    "top_user=db.nashville.aggregate(pipeline1)[\"result\"][0][\"_id\"]\n",
    "top_user_count=db.nashville.aggregate(pipeline1)[\"result\"][0][\"count\"]\n",
    "print(\"Top contributing user\\t\\t:{0}, (no. of entries:{1})\".format(top_user, top_user_count))\n",
    "# Number of contributing users appearing only once (having 1 post)\n",
    "pipeline2 = [\n",
    "        {\"$group\":{\"_id\":\"$created.user\",\n",
    "                    \"count\":{\"$sum\":1}}},\n",
    "        {\"$group\":{\"_id\":\"$count\",\"num_users\":{\"$sum\":1}}},\n",
    "        {\"$sort\" :{\"_id\":1}},\n",
    "        {\"$limit\":1}\n",
    "        ]\n",
    "one_users=db.nashville.aggregate(pipeline2)[\"result\"][0][\"num_users\"]\n",
    "print(\"Number of contributing users appearing only once\\t:\",one_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ok': 1.0,\n",
      " 'result': [{'_id': 'node', 'count': 1317086},\n",
      "            {'_id': 'way', 'count': 135617},\n",
      "            {'_id': 'relation', 'count': 1897}]}\n"
     ]
    }
   ],
   "source": [
    "# Number of entries based on type\n",
    "pipeline = [\n",
    "        {\"$group\":{\"_id\":\"$type\",\n",
    "                    \"count\":{\"$sum\":1}}},\n",
    "        {\"$sort\":{\"count\":-1}}\n",
    "        ]\n",
    "pprint.pprint(db.nashville.aggregate(pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributor Statistics and Suggestions to improve data content and quality\n",
    "### Contributor Statistics\n",
    "\n",
    "Top 3 users `(woodpeck_fixbot, Shawn Noble, st1974)` contribute bulk of the data accounting for 37.47% of the data. All the remaining users contribute lesser than 5% of the data suggesting almost equal contributions. \n",
    "\n",
    "\n",
    "### Suggestions to imporve user contribution and fill missing data\n",
    "\n",
    "\n",
    "1. To further the user contributions, it is suggested that each user get paid a small amount (eg: 2 cents) for each additional data which can be verified by another user(s) (for validity of the data). The monetary cost for payment to contributers can be recovered by giving a paid premium version of the data. The customer with access to premium version can recover the cost paid by extracting value out of premium services to further his bussiness model. Since most of the data is available openly, it is imperative that the premium data services include significant advantage to the entities subscribing the service.\n",
    "    * **Anticipated problems** :\n",
    "        * One user can add unverified data(manually or through online-robots/scripts) and the other approve unverified data since they both benefit from the payment.\n",
    "        * Since amenities can be closed down and moved to another area/building it is also problematic to re-enlist new location and updating old ones.\n",
    "\n",
    "2. Additionally, points (benefits) to popular games can be offered as an incentive to improve dataset. For example, a tie-up with the game \"PokemanGo\" can offer free poke-points to gamers who can contribute data. Such a system allows \"PokemanGo\" improve their services by introducing poke-stops at more convincing places.\n",
    "\n",
    "3. Grouping the nodes based on the co-ordinates of county boundaries can be used to input the `address:county` field in the data set. Also some of the fields in the dataset has to be validated with publicly available verified data for the accuracy of the given dataset.\n",
    "    * **Anticipated problems** : \n",
    "        * Getting verified public data for remote areas or areas which has not been surveyed before can be challenging.\n",
    "        \n",
    "4. Laws can be passed for mandatory listing of essential public services such as police stations, courts, hospitals etc in atleast 2 different maping systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': 'woodpeck_fixbot', 'count': 278223},\n",
      " {'_id': 'Shawn Noble', 'count': 170523},\n",
      " {'_id': 'st1974', 'count': 96271},\n",
      " {'_id': 'AndrewSnow', 'count': 55606},\n",
      " {'_id': 'Rub21', 'count': 53880}]\n",
      "\n",
      "woodpeck_fixbot\t:19.13%\n",
      "Shawn Noble\t:11.72%\n",
      "st1974\t\t:6.62%\n",
      "AndrewSnow\t:3.82%\n",
      "Rub21\t\t:3.7%\n"
     ]
    }
   ],
   "source": [
    "db=get_db(\"openstreetmap\")\n",
    "# Top 5 contributing users and their percentage\n",
    "pipeline = [\n",
    "        {\"$group\":{\"_id\":\"$created.user\",\n",
    "                    \"count\":{\"$sum\":1}}},\n",
    "        {\"$sort\" :{\"count\":-1}},\n",
    "        {\"$limit\":5}\n",
    "        ]\n",
    "pprint.pprint(list(db.nashville.aggregate(pipeline)))\n",
    "print('\\nwoodpeck_fixbot\\t:{0}%'\n",
    "      '\\nShawn Noble\\t:{1}%'\n",
    "      '\\nst1974\\t\\t:{2}%'\n",
    "      '\\nAndrewSnow\\t:{3}%'\n",
    "      '\\nRub21\\t\\t:{4}%'.format(round(278223*100/1454600,2),\n",
    "                             round(100*170523/1454600,2),\n",
    "                             round(96271/1454600*100,2),\n",
    "                             round(100*55606/1454600,2),\n",
    "                             round(100*53880/1454600,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional data exploration using MongoDB queries - Missing address for amenities\n",
    "\n",
    "Further `db` queries using shows that there are number of entries (8167 entries) lacking an address for the amenity specified. By grouping, we found that `amenity:grave_yard` has maximum number of missing address entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ok': 1.0, 'result': [{'amenities_with_no_address': 8167}]}\n"
     ]
    }
   ],
   "source": [
    "# Number of nodes with amenities but no address\n",
    "pipeline = [\n",
    "        {\"$match\":{\"type\":\"node\",\n",
    "                   \"amenity\":{\"$exists\":1},\n",
    "                   \"address\":{\"$exists\":0}}},\n",
    "        {\"$count\":\"amenities_with_no_address\"}\n",
    "        ]\n",
    "pprint.pprint(db.nashville.aggregate(pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ok': 1.0,\n",
      " 'result': [{'_id': 'grave_yard', 'count': 3469},\n",
      "            {'_id': 'place_of_worship', 'count': 2351},\n",
      "            {'_id': 'school', 'count': 1356},\n",
      "            {'_id': 'restaurant', 'count': 179},\n",
      "            {'_id': 'fast_food', 'count': 105}]}\n"
     ]
    }
   ],
   "source": [
    "# Number of nodes with amenities(grouped) but no address\n",
    "pipeline = [\n",
    "        {\"$match\":{\"type\":\"node\"}},\n",
    "        {\"$match\":{\"amenity\":{\"$exists\":1}}},\n",
    "        {\"$match\":{\"address\":{\"$exists\":0}}},\n",
    "        {\"$group\":{\"_id\":\"$amenity\",\n",
    "                    \"count\":{\"$sum\":1}}},\n",
    "        {\"$sort\" :{\"count\":-1}},\n",
    "        {\"$limit\":5}\n",
    "        ]\n",
    "pprint.pprint(db.nashville.aggregate(pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional queries\n",
    "### Number of references to nodes in each tag type:\n",
    "\n",
    "It is found that most of the references to the nodes are associated with `way` type ( > 98%) than `relation` type ( < 2%).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pymongo.command_cursor.CommandCursor object at 0x10646cc50>\n",
      "\n",
      "way\t\t:98.94%\n",
      "relation\t:1.06%\n"
     ]
    }
   ],
   "source": [
    "# Number of references in each node type\n",
    "pipeline = [\n",
    "        {\"$unwind\"  :\"$ref\"},\n",
    "        {\"$group\":{\"_id\":\"$type\",\n",
    "                    \"count\":{\"$sum\":1}}},\n",
    "        {\"$sort\" :{\"count\":-1}}\n",
    "        ]\n",
    "print(db.nashville.aggregate(pipeline))\n",
    "total=1422876+15216\n",
    "print('\\nway\\t\\t:{0}%'\n",
    "      '\\nrelation\\t:{1}%'.format(round(1422876*100/total,2),\n",
    "                                round(15216*100/total,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 amenities:\n",
    "\n",
    "It is shown that `grave_yard` is the top amenity [largest no. of entries] mentioned in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ok': 1.0,\n",
      " 'result': [{'_id': 'grave_yard', 'count': 3488},\n",
      "            {'_id': 'place_of_worship', 'count': 2512},\n",
      "            {'_id': 'parking', 'count': 1561},\n",
      "            {'_id': 'school', 'count': 1504},\n",
      "            {'_id': 'restaurant', 'count': 374}]}\n"
     ]
    }
   ],
   "source": [
    "# Top 5 amenities occuring in the data\n",
    "pipeline = [\n",
    "        {\"$match\":{\"amenity\":{\"$exists\":1}}},\n",
    "        {\"$group\":{\"_id\":\"$amenity\",\n",
    "                    \"count\":{\"$sum\":1}}},\n",
    "        {\"$sort\" :{\"count\":-1}},\n",
    "        {\"$limit\":5}\n",
    "        ]\n",
    "pprint.pprint(db.nashville.aggregate(pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commonly occuring area codes:\n",
    "\n",
    "The most comonly occuring area codes are identified: `931` and `615`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'count': 459, '_id': '931'}, {'count': 263, '_id': '615'}]\n"
     ]
    }
   ],
   "source": [
    "# 2 most commonly occuring area codes\n",
    "pipeline = [\n",
    "        {\"$unwind\"  :\"$phone\"},\n",
    "        {\"$group\":{\"_id\":\"$phone\",\n",
    "                    \"count\":{\"$sum\":1}}},\n",
    "        {\"$sort\" :{\"count\":-1}},\n",
    "        {\"$limit\":2}\n",
    "        ]\n",
    "print(list(db.nashville.aggregate(pipeline)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "A thorough review of the data wrangling process is shown along with inserting cleaned data into MongoDB database. All required queries are performed along with new additional queries. We have also included certain suggestions on improving the missing data and also to monetize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. Python Documentation https://docs.python.org/3.4/index.html\n",
    "2. ElementTree Overview http://effbot.org/zone/element-index.htm\n",
    "3. MongoDb Documentation https://docs.mongodb.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
